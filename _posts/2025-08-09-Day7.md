---
title: "LLM Fine-tuning: Day 7"
date: 2025-08-09
categories: [LLM Fine-tuning]
---

# Full Training on Colabs and Testing Prep

**Colabs**

Today, I managed to get a free Colabs Pro subscription through their student verification process, meaning that I have access to better GPUs.

Instead of using T4 (which I constant had an issue with), I decided to use A100 to reduce my training duration significantly.

<img width="565" height="414" alt="Screenshot 2025-08-09 at 10 55 48 PM" src="https://github.com/user-attachments/assets/e763f9b8-3f33-4ccb-94c7-1ccf724541bf" />

I revised my training script to optimize it for the A100 GPU and utilize most of 40GB of GPU provided. So, I set the script to use 0.95 (95%) of the GPU provided.

<img width="636" height="205" alt="Screenshot 2025-08-09 at 10 59 29 PM" src="https://github.com/user-attachments/assets/92f782b3-2d23-482f-8287-953166b6c578" />

**Setting Up for Training**

Then, I set up the GPT-2 124 parameter model, its tokenizer, and my special tokens to prepare for training.

<img width="617" height="261" alt="Screenshot 2025-08-09 at 11 00 39 PM" src="https://github.com/user-attachments/assets/261e5893-fb85-4a8f-8681-f4450ae59bef" />

As usual, I initialzed Weights and Biases to mointor the training loss, validation loss, and learning rate.

<img width="629" height="280" alt="Screenshot 2025-08-09 at 11 02 03 PM" src="https://github.com/user-attachments/assets/b8d32d4a-3f5e-4269-a5ba-08b86ce489a6" />

After that, I loaded my full tokenized datasets and printed out how many I had for tracking purposes.

<img width="751" height="216" alt="Screenshot 2025-08-09 at 11 16 08 PM" src="https://github.com/user-attachments/assets/65133f75-229d-42ce-b23d-1ce7f23182d4" />

With all of that set, I wrote my training arguments to fully utilize the A100 GPU's capabilities. 
- Increased the batch size to 32 and reduced the gradient accumulation steps to 2
- Increased epoch to 5 for better quality results and dataloader workers to 16
- Enhanced learning rate to 5e-4 and set up weight decay to prevent overfitting
- Set up methods for logging training/validation loss

<img width="615" height="621" alt="Screenshot 2025-08-09 at 11 20 02 PM" src="https://github.com/user-attachments/assets/2325c755-3de8-43ff-9112-bb83de1bf309" />

**Training Results**

I successfully ran the script and the training process only took around 2.5 hours, which is a significant reduction from ~5 hours using the T4 GPU.

<img width="1142" height="412" alt="Screenshot 2025-08-09 at 11 20 49 PM" src="https://github.com/user-attachments/assets/bbe80830-2183-40ff-99a7-0b077d4d1a36" />

Looking at my wandb panels, it's clear that my training process went well without any issues because you can see that both the training and validation losses increased with more datasets, meaning that model became more accurate. Also, the decay seen in the learning rate showcases improved convergence towards an optimized solution

<img width="500" alt="Training Loss Graph" src="https://github.com/user-attachments/assets/0cb8828f-72e1-4c51-a7d0-02e59caf9b35" />
<img width="500" alt="Validation Loss Graph" src="https://github.com/user-attachments/assets/4f50dab4-5b9f-4d9d-ac41-d476a972acdb" />
<img width="500" alt="Learning Rate Graph" src="https://github.com/user-attachments/assets/51835423-99de-46cd-ab8f-029c4fdc82c5" />

**Moving On**

I will work on creating simple test to evaluate the quality of the model's output and eventually test it on all the codes in my testing dataset.

If the model performs with at least a 80% accuracy, I will deploy it on hugging face and use API calls to make a simple web application for use.

