---
title: "LLM Fine-tuning: Day 4"
date: 2025-07-14
categories: [LLM Fine-tuning]
---

# Testing GPT-2 Set Up & Test Training

Today, I worked on setting up my GPT-2 model and tested training it with a small sample to ensure that everything works before full training.

**GPT-2 Set Up**

I set up the model by loading it and its pre-trained tokenizer. The model will run on my computer's cpu because I don't have a CUDA (GPU) available at the moment.

<img width="626" height="571" alt="Screenshot 2025-07-14 at 9 51 43 PM" src="https://github.com/user-attachments/assets/d4b3c872-1f7d-4aec-b14d-d2fe9128603a" />

Then, I created a simple input format to train the model with one example.

<img width="551" height="681" alt="Screenshot 2025-07-14 at 9 53 21 PM" src="https://github.com/user-attachments/assets/22ac9679-8aa0-4d4c-9322-55d4c922543a" />

Thankfully, the model produced the expected outcome without any errors, giving me the green light to move on to pre-training.

**Test Training**

First, I loaded a small subset (100 examples) of data to train my gpt-2 model.

<img width="569" height="199" alt="Screenshot 2025-07-14 at 9 54 32 PM" src="https://github.com/user-attachments/assets/1aacffd6-da9a-4e3f-bcb2-f80ec587d8e3" />

Then, I loaded the model and added two special tokenizers (padding and separator) to ensure that the formatting is uniform.

<img width="457" height="355" alt="Screenshot 2025-07-14 at 9 55 04 PM" src="https://github.com/user-attachments/assets/0ba8d334-5c24-44cd-bb6d-48b27acd1c20" />

For the training arguements, I set the epoch to 1 and learning rate to 5e-5 (which is the slowest rate) since the model is running on my computer's cpu.

<img width="466" height="313" alt="Screenshot 2025-07-14 at 9 55 54 PM" src="https://github.com/user-attachments/assets/af3a2df8-630b-44ab-9b6f-1b7d1eda95a0" />

After creating the trainer, I moved onto testing the output extraction. I used a low temperature for this test so that the model is more concise dealing with a small subset of data.

<img width="678" height="573" alt="Screenshot 2025-07-14 at 9 58 24 PM" src="https://github.com/user-attachments/assets/c59e6c2c-1d8c-4104-9271-b988f91f13a9" />

I also had this test to be ran on wandb to visualize how my model is being trained. The downward slope of both the eval and train loss graphs indicate that the model is generalizing well to unseen data and fits the training data well.

<img width="600" alt="W B Chart 7_14_2025, 10_04_24 PM" src="https://github.com/user-attachments/assets/da1e0bc4-a5b7-458d-88d4-92cccec59354" />
<img width="600" alt="W B Chart 7_14_2025, 10_04_32 PM" src="https://github.com/user-attachments/assets/3ff60661-e218-4ee7-a0bd-5193394df077" />

That's it for today, and I will work on full training moving on.










