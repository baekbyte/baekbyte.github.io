---
title: "LLM Fine-tuning: Day 8 (END)"
date: 2025-08-10
categories: [LLM Fine-tuning]
---

# Testing 

First, I mounted my model back on to my colabs run time because colabs wipes everything out when a session ends.

<img width="1241" height="518" alt="Screenshot 2025-08-10 at 4 09 12 PM" src="https://github.com/user-attachments/assets/978b8f95-85e5-4ab0-952a-915ef5311d10" />

After that, I developed a script a test the model across 10 examples. It performd horribly with an average similarity score of 20%.

<img width="1137" height="677" alt="Screenshot 2025-08-10 at 4 10 14 PM" src="https://github.com/user-attachments/assets/9f37c48a-28f8-477f-a95f-f15df22fb147" />

I knew that the model itself wasn't the problem due to previous comprehensive debugging so I examined my training data and found an issue. 

<img width="959" height="526" alt="Screenshot 2025-08-10 at 4 11 45 PM" src="https://github.com/user-attachments/assets/b6094938-9658-4bcf-b4ae-3920b8ba0e84" />

The training dataset had chuncks of meaning less text and a few of them repeated "<|endoftext|>" more than a hundred time. Since I couldn't clean up the training data due to its arbitrary manner of meaningless text, I retrained the model on a smaller dataset with better organization (python_code_instructions_18k_alpaca)

<img width="654" height="517" alt="Screenshot 2025-08-10 at 4 14 29 PM" src="https://github.com/user-attachments/assets/69185186-271c-4392-8e4f-21c322293e91" />

The training went well and it only took 17 minutes since we only had 18k data. But, this also means that the model wouldn't perform as well.

After that, I tested the model on 28 comprehensive examples, and it performed significantly better althought it's similarity rates were still low.

<img width="640" height="386" alt="Screenshot 2025-08-10 at 4 16 02 PM" src="https://github.com/user-attachments/assets/bc6a7069-3bbe-47d2-aaaa-f5410d7cbec6" />

In the end, this was a training data issue. I tried looking for larger datasets with better structuring but I find any that were open source. So, this will be the end of this project and I won't be deploying my model to hugging face.

Lessons-leared:
- How to tokenize datasets and call them from huggingface
- How to train a model (training args)
- How to test a model
- How to utilize wandb

*Now I'll move on to my next project on building a technical application for comprehensive alignment testing*

